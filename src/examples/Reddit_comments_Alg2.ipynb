{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c253640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/heydar/me/BSC/FinalPorject/lsh\n"
     ]
    }
   ],
   "source": [
    "%cd /home/heydar/me/BSC/FinalPorject/lsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe63957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5fa06",
   "metadata": {},
   "source": [
    "# Loading Dataset\n",
    "## Preparing Dataset from sickit-learn\n",
    "In this example our dataset is a news group dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1184afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715b2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('/media/heydar/extra2/BSC/LSH_DATASET/Reddit_Big/comments_positive.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00fe92c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca1d1573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This isn't Twitter: try to comment on the arti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, it is exactly what it sounds like. It's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In soviet Russia, bomb disarms you!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"runin for senitur! #YOLO!\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You step motherfucker.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999995</th>\n",
       "      <td>yeah man, those things are bad for your health.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999996</th>\n",
       "      <td>I honestly like this because it asks the other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999997</th>\n",
       "      <td>Is... that what you see here?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999998</th>\n",
       "      <td>Prepare to lose your breakfast. Entire school ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999999</th>\n",
       "      <td>No doubt that it seems weird and dumb that peo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text\n",
       "0        This isn't Twitter: try to comment on the arti...\n",
       "1        Well, it is exactly what it sounds like. It's ...\n",
       "2                      In soviet Russia, bomb disarms you!\n",
       "3                              \"runin for senitur! #YOLO!\"\n",
       "4                                   You step motherfucker.\n",
       "...                                                    ...\n",
       "1999995    yeah man, those things are bad for your health.\n",
       "1999996  I honestly like this because it asks the other...\n",
       "1999997                      Is... that what you see here?\n",
       "1999998  Prepare to lose your breakfast. Entire school ...\n",
       "1999999  No doubt that it seems weird and dumb that peo...\n",
       "\n",
       "[2000000 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data=data[['id','body']]\n",
    "\n",
    "data=data.loc[data['text'] != '[removed]']\n",
    "data[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "890cb263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/heydar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/heydar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/heydar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0081846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text(text):\n",
    "    \n",
    "    text=str(text)\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation and non-alphanumeric characters\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    normalized_text = ' '.join(words)\n",
    "    return normalized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8a48551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'twitter try comment article current activity'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text(part1['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c75c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "part1=data.iloc[:500000]\n",
    "part2=data.iloc[500000:1000000]\n",
    "part3=data.iloc[1000000:1500000]\n",
    "part4=data.iloc[1500000:2000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a828873d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44929/1403953296.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  part1['text']=part1['text'].apply(lambda x : normalize_text(x))\n"
     ]
    }
   ],
   "source": [
    "part1['text']=part1['text'].apply(lambda x : normalize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f22776a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44929/384413117.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  part2['text']=part2['text'].apply(lambda x : normalize_text(x))\n"
     ]
    }
   ],
   "source": [
    "part2['text']=part2['text'].apply(lambda x : normalize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "187bd815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44929/2976579892.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  part3['text']=part3['text'].apply(lambda x : normalize_text(x))\n"
     ]
    }
   ],
   "source": [
    "part3['text']=part3['text'].apply(lambda x : normalize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0799182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44929/2701164848.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  part4['text']=part4['text'].apply(lambda x : normalize_text(x))\n"
     ]
    }
   ],
   "source": [
    "part4['text']=part4['text'].apply(lambda x : normalize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c6af6b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([part1,part2,part3,part4]).to_csv('/media/heydar/extra2/BSC/LSH_DATASET/Reddit_Big/normalized_positive.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b94598",
   "metadata": {},
   "source": [
    "### Do TF-IDF on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcc0e214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heydar/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3457: DtypeWarning: Columns (0,1,2,3,4,5,8,10,11,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('/media/heydar/extra2/BSC/LSH_DATASET/Reddit_Big/normalized_positive.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b65541",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cf9184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b08cc4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>twitter try comment article current activity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>well exactly sound like shoebox least whenever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>soviet russia bomb disarms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>runin senitur yolo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>step motherfucker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262320</th>\n",
       "      <td>yeah man thing bad health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262321</th>\n",
       "      <td>honestly like asks gender opinion pervious top...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262322</th>\n",
       "      <td>see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262323</th>\n",
       "      <td>prepare lose breakfast entire school district ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262324</th>\n",
       "      <td>doubt seems weird dumb people seem gloat busy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1977004 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text\n",
       "0             twitter try comment article current activity\n",
       "1        well exactly sound like shoebox least whenever...\n",
       "2                               soviet russia bomb disarms\n",
       "3                                       runin senitur yolo\n",
       "4                                        step motherfucker\n",
       "...                                                    ...\n",
       "2262320                          yeah man thing bad health\n",
       "2262321  honestly like asks gender opinion pervious top...\n",
       "2262322                                                see\n",
       "2262323  prepare lose breakfast entire school district ...\n",
       "2262324  doubt seems weird dumb people seem gloat busy ...\n",
       "\n",
       "[1977004 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc42608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    min_df=5,\n",
    "    ngram_range=(3,3),\n",
    "    stop_words=\"english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f63c07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorization done in 54.265 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "X_tfidf = vectorizer.fit_transform(data.text)\n",
    "\n",
    "print(f\"vectorization done in {time() - t0:.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8337d2",
   "metadata": {},
   "source": [
    "after adding limit to data some of rows are empty so we need to drop them for better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "124a8d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "drops_datas=np.where([X_tfidf.getnnz(1)<1])[1]\n",
    "nnzero_rows=X_tfidf.getnnz(1)>1\n",
    "X_tfidf=X_tfidf[nnzero_rows]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20f810ca",
   "metadata": {},
   "source": [
    "In this section, we need to shigling, we do this shingling y using th e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61186667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the CSR matrix and convert it to a COO matrix\n",
    "coo_matrix = X_tfidf.tocoo(copy=True)\n",
    "\n",
    "# Replace non-zero values with their own column indices\n",
    "coo_matrix.data = coo_matrix.col\n",
    "\n",
    "# Convert the COO matrix back to CSR format\n",
    "shingles = coo_matrix.tocsr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bb8c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set Cluster Number\n",
    "true_k = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a89e19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167380, 75931)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21fedf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for Create LSH index\n",
      "LSH index Created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.LSH.MinHash import MinHash\n",
    "from src.LSH.lsh import LSH\n",
    "#Make LSH\n",
    "prime=75931\n",
    "lsh_time_start=time()\n",
    "b, r = 1, 1\n",
    "num_perm = b * r\n",
    "minhash = MinHash(num_perm)\n",
    "mh = minhash.sign_csr_callback_permutation(shingles, shingles.shape[1],prime)\n",
    "mh = np.array(mh)\n",
    "print('waiting for Create LSH index')\n",
    "# Create LSH index\n",
    "lsh = LSH(b=b, r=r)\n",
    "lsh.fit(mh)\n",
    "print('LSH index Created')\n",
    "lsh_time=time()-lsh_time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82c53d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "evaluations = []\n",
    "evaluations_std = []\n",
    "\n",
    "\n",
    "def fit_and_evaluate(km, X,lsh=None, name=None, n_runs=5,seek_time=0):\n",
    "    name = km.__class__.__name__ if name is None else name\n",
    "\n",
    "    train_times = []\n",
    "    scores = defaultdict(list)\n",
    "    for seed in range(n_runs):\n",
    "        km.random_state=seed\n",
    "        t0 = time()\n",
    "        if lsh:\n",
    "            km.fit(X,lsh)\n",
    "        else:\n",
    "            km.fit(X)\n",
    "        train_times.append(time()+seek_time - t0)\n",
    "        scores['inertia'].append(km.inertia_)\n",
    "        scores[\"Silhouette Coefficient\"].append(\n",
    "            metrics.silhouette_score(X, km.labels_, sample_size=10000)\n",
    "        )\n",
    "    train_times = np.asarray(train_times)\n",
    "\n",
    "    print(f\"clustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s \")\n",
    "    evaluation = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.mean(),\n",
    "    }\n",
    "    evaluation_std = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.std(),\n",
    "    }\n",
    "    for score_name, score_values in scores.items():\n",
    "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
    "        print(f\"{score_name}: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "        evaluation[score_name] = mean_score\n",
    "        evaluation_std[score_name] = std_score\n",
    "    evaluations.append(evaluation)\n",
    "    evaluations_std.append(evaluation_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc79dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5aad520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_centroids=KMeans(\n",
    "    n_clusters=true_k,\n",
    "    max_iter=1,\n",
    "    n_init=1,\n",
    "    init='k-means++'\n",
    ").fit(X_tfidf).cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: initializing centers\n",
      "Init: initializing clusters\n"
     ]
    }
   ],
   "source": [
    "from src.Kmeans.mh_kmeans import MHKmeans\n",
    "\n",
    "mh_kmeas=MHKmeans(\n",
    "    n_clusters=true_k,\n",
    "    max_iteration=1,\n",
    "    n_init=1,\n",
    "    verbose=True,\n",
    "    \n",
    "    init=init_centroids\n",
    ")\n",
    "fit_and_evaluate(mh_kmeas, X_tfidf,lsh, name=\"Mh KMeans random \\non  tf-idf vectors\", n_runs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0effb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Kmeans.kmeans import Kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a55247",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_kmeans = Kmeans(\n",
    "    n_clusters=true_k,\n",
    "    max_iteration=1,\n",
    "    n_init=1,\n",
    "    verbose=True,\n",
    "    \n",
    "    init=init_centroids\n",
    ")\n",
    "\n",
    "fit_and_evaluate(simple_kmeans, X_tfidf,None, name=\"KMeans random \\non  tf-idf vectors\", n_runs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e21a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "fig, (ax0, ax1,ax2) = plt.subplots(ncols=3, figsize=(16, 6), sharey=True)\n",
    "\n",
    "df = pd.DataFrame(evaluations[::-1]).set_index(\"estimator\")\n",
    "df_std = pd.DataFrame(evaluations_std[::-1]).set_index(\"estimator\")\n",
    "\n",
    "df.drop(\n",
    "    [\"train_time\",'inertia'],\n",
    "    axis=\"columns\",\n",
    ").plot.barh(ax=ax0, xerr=df_std)\n",
    "ax0.set_xlabel(\"Clustering scores\")\n",
    "ax0.set_ylabel(\"\")\n",
    "\n",
    "df[\"train_time\"].plot.barh(ax=ax1, xerr=df_std[\"train_time\"])\n",
    "ax1.set_xlabel(\"Clustering time (s)\")\n",
    "\n",
    "\n",
    "df[['inertia']].plot.barh(ax=ax2,xerr=df_std)\n",
    "ax2.set_xlabel(\"Clustering cost\")\n",
    "ax2.set_ylabel(\"\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"RedditBig_500_1_1Algo2.png\")\n",
    "plt.savefig('RedditBig_500_1_1Algo2.pdf')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
